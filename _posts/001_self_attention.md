---
layout: post
title: Blogs
permalink: /blogs/001/
---

### Transformer Model: Self Attentios: Implementation with In-Depth Details

#### Implementation
Following is the basic implementation of Self-Attention using Pytorch. 
The main goal of this implementation is to make it easier to understand that how the 
attention score is computed and is used on values to generate the final output. 
The optimized implementation of multiheaded-attention with einsum() will follow in next one.

<script src="https://gist.github.com/makeesyai/e36430f95cabca165a384fa77519d398.js"></script>
